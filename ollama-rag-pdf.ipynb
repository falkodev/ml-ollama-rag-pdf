{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5fb7a2",
   "metadata": {},
   "source": [
    "## Ingesting PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e8b999-83ba-484a-9b94-f56c201d2036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q unstructured langchain\n",
    "%pip install --upgrade --quiet  \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0e2f74-7c4b-4665-8d87-bc00656f31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104c0b18-1c06-41a1-a2ca-f9ee23f4f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"AI_Anxiety_V4.pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if local_path:\n",
    "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "  data = loader.load()\n",
    "else:\n",
    "  print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38119195-9c91-4e58-aa46-8a74244032af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/374092896\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nArticle in SSRN Electronic Journal · September 2023\\n\\nDOI: 10.2139/ssrn.4573394\\n\\nCITATIONS 2\\n\\nREADS 1,749\\n\\n11 authors, including:\\n\\nJeff Kim\\n\\nShrinidhi Kadkol\\n\\nUniversity of Illinois at Chicago\\n\\nUniversity of Illinois at Chicago\\n\\n7 PUBLICATIONS 11 CITATIONS\\n\\n8 PUBLICATIONS 46 CITATIONS\\n\\nSEE PROFILE\\n\\nSEE PROFILE\\n\\nOlusola Ajilore\\n\\nUniversity of Illinois at Chicago\\n\\n254 PUBLICATIONS 3,934 CITATIONS\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Jeff Kim on 12 November 2023.\\n\\nThe user has requested enhancement of the downloaded file.\\n\\nAI ANXIETY: A COMPREHENSIVE ANALYSIS OF PSYCHOLOGICAL FACTORS AND INTERVENTIONS\\n\\nA PREPRINT\\n\\nJeff J.H. Kim Department of Biomedical Engineering University of Illinois College of Medicine Chicago, IL, USA jkim671@uic.edu\\n\\nShrinidhi Kadkol Department of Physiology and Biophysics University of Illinois College of Medicine Chicago, IL, USA skadko2@uic.edu\\n\\nItay Solomon Department of Biochemistry and Molecular Genetics University of Illinois College of Medicine Chicago, IL, USA isolom2@uic.edu\\n\\nHyelin Yeh Department of Medicine Hanyang University College of Medicine Seoul, Republic of Korea dazzlingblue@hanyang.ac.kr\\n\\nJun Young Soh Department of Mental Health Johns Hopkins Bloomberg School of Public Health Baltimore, MD, USA jsoh2@jh.edu\\n\\nTheresa M. Nguyen Department of Biomedical Engineering University of Illinois College of Medicine Chicago, IL, USA tnguy271@uic.edu\\n\\nJeong Yun Choi Department of Communication Seoul National University Seoul, Republic of Korea jychoi15@snu.ac.kr\\n\\nSophie Lee Department of Mental Health Johns Hopkins Bloomberg School of Public Health Baltimore, MD, USA slee503@jh.edu\\n\\nAdith V. Srivatsa Department of Biomedical Engineering University of Illinois College of Medicine Chicago, IL, USA adithvs2@uic.edu\\n\\nGeorgie R. Nahass Department of Biomedical Engineering University of Illinois College of Medicine Chicago, IL, USA gnahas2@uic.edu\\n\\nOlusola Ajilore Department of Psychiatry University of Illinois at Chicago Chicago, IL, USA oajilore@uic.edu\\n\\nNovember 12, 2023\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nABSTRACT\\n\\nThe rapid advancement of artificial intelligence (AI), including flexible systems like generative AI, has raised concerns about its psychological impacts, necessitating a multidisciplinary approach to address AI-induced anxiety. This article systematically investigates AI anxiety’s unique stressors across different age groups – young adults, middle-aged, and seniors – and delineates common causes such as privacy concerns, AI-generated misinformation, uncontrolled AI development, and inherent biases. While focusing on non-clinical aspects of AI anxiety, this article delves into its extensive effects on mental and physical well-being, examining the consequent implications for healthcare costs, economic productivity, and fertility rates. To address AI anxiety effectively, the article proposes multidisciplinary solutions. First, the paper highlights the urgent need for AI-adapted clinical guidelines, incorporating tailored diagnostic and therapeutic measures aimed at mitigating AI anxiety. It also emphasizes the role of continuous education in demystifying and adapting to the age of AI, advocating for prompt updates to school curricula to match the rapid progress in AI technology. Emphasis is given to promoting regulatory measures to balance AI innovation and societal adaptability, including engineering constraints and ethical design as essential components of responsible AI development. Lastly, it explores pioneering solutions to mitigate AI anxiety, such as leveraging AI in mental health interventions. The article concludes that an intensified focus and strategic resource allocation are imperative to address the escalating mental health challenges as society navigates the impending era of pervasive AI.\\n\\nKeywords Artificial Intelligence, Anxiety, AI Anxiety, Generative AI, ChatGPT, Mental Health\\n\\n2\\n\\nA PREPRINT\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n1 Introduction\\n\\nArtificial Intelligence (AI) is a multifaceted domain within computer science focused on developing systems that can simulate human intelligence in machines.[1] Introduced in the 1950s, AI has undergone significant evolutions, profoundly impacting various sectors including robotics and virtual assistants. Among the diverse branches of AI, generative AI has recently garnered attention for its ability to swiftly generate new content, be it text or images.[2] This notable advancement relies on probabilistic methods to anticipate subsequent items in a sequence by interpreting patterns within its dataset. This prediction-based approach enables a unique level of adaptability and flexibility, allowing these AI systems to generate contextually relevant and novel content.[3] ChatGPT, developed by OpenAI, is at the forefront of these generative AI models. It utilizes an expansive body of text data to grasp the subtleties of human language and employs deep learning methods to craft human-like responses, allowing it to produce suitable answers to a wide array of prompts. [4]\\n\\nBefore the advent of generative AI, artificial intelligence operated largely within the boundaries of task-specific algorithms, designed to excel in predetermined domains.[5] These early forms of AI, known as narrow AI, were limited in their adaptability and versatility, as their functionality was confined to the tasks for which they were specifically programmed. Examples include AI for image recognition, voice assistants capable of executing a limited set of commands, or AI utilized in predictive modeling for weather forecasting.[6, 7, 8, 9]\\n\\nThe integration of machine learning has been pivotal in transforming the landscape of AI, as it enabled algorithms to derive insights and make predictions based on data.[10] Despite this advancement, these models largely depended on extensive handcrafted features and labeled training datasets, requiring significant human input for their functionality.[11] The advent of deep learning marked an important turning point in the field of AI, leveraging neural networks with many layers to process and learn from vast amounts of data.[12] However, early deep learning models were often described as \"black boxes\" because their internal workings and decision-making processes were not easily decipherable. This lack of transparency posed a challenge in understanding how these models formulated their conclusions, which in turn affected their broader applicability in areas where clear and explicit reasoning was required.[13, 14]\\n\\nThe advent of generative AI has marked a significant paradigm shift within the realm of artificial intelligence, transi- tioning from narrowly focused, task-specific models to more versatile, multi-purpose systems.[15] The introduction of advanced architectures, such as Generative Adversarial Networks (GANs), has been pivotal in showcasing their capacity to generate realistic images, music, and even textual content.[16] As these models evolved, they infiltrated many sectors. Healthcare was transformed through synthetic biomedical data generation and disease progression simulation while the entertainment industry was enriched with lifelike graphics for video games and original music scores.[17, 18, 19]\\n\\nThis transformation in the capabilities of generative AI technologies has led to considerable excitement within the venture capital landscape. This interest has materialized into a considerable influx of investments into companies specializing in generative AI.[20] In the first half of 2023 alone, $15.2 billion was invested in generative AI companies, leading to the birth of a number of startups aimed at developing AI-based products.[21] Looking forward, the abundance of venture capital funding in this field is expected to fuel the expansion and sophistication of flexible AI in the coming years. As a result, AI is anticipated to become more integrated into our daily lives and work, further expanding its footprint across different industries.\\n\\nVenture capital is not the only sector showing interest in the development of generative AI. Following the introduction of ChatGPT, tech titans such as Google, Apple, Samsung, Meta, and Amazon have rallied their resources to cultivate their own proprietary language learning models.[22, 23] This surge in development signifies a shared acknowledgement of generative AI’s potential and the instrumental role it will play in sculpting the future of technology.\\n\\nFurthermore, the realm of generative AI is merely in its infancy. We stand on the precipice of a new era where generative AI will seep into multiple facets of our lives, from content creation to personal assistance to customer service. However, generative AI is just the beginning of a future for flexible and adaptable AI systems. With the pace of technological advancement, we anticipate the birth of AI systems that not only parallel but outperform human capabilities in an ever-expanding range of fields. This progressive shift promises extensive ripple effects, revolutionizing our interaction with technology and presenting both unforeseen challenges and opportunities.\\n\\n3\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nFigure 1: Timeline of Key Milestones in Artificial Intelligence (AI): This graphical timeline traces the seminal events and technological breakthroughs of AI, from its conceptual inception in the 1950s to the latest advance- ments in generative models in the 2020s. The timeline highlights key advancements in adaptable AI systems, challenges to human proficiency, and growing concerns about AI’s influence.\\n\\n4\\n\\nA PREPRINT\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n2 AI-induced Anxiety:\\n\\nFigure 2: Comparative Analysis of Automation Anxiety vs. AI Anxiety: A side-by-side examination of the concerns originating from traditional automation and the emerging complexities introduced by AI-driven systems.\\n\\nAI anxiety is not a novel phenomenon. Concerns regarding the rapid advancement of disruptive technologies dates back to the earlier periods of industrial revolution, wherein manual labor was gradually replaced by machinery, generating a fear known as automation anxiety.[24] In essence, both AI and automation anxiety reflect apprehensions about the unknown implications of novel technologies, predominantly pertaining to potential job displacement and the obfuscation of human roles.\\n\\nHowever, the narrative diverges significantly when we consider the rise of flexible AI systems. Previous instances of automation anxiety centered primarily on the replacement of manual and repetitive tasks, a transformation largely confined to the physical realm. In contrast, the ability of generative AI and flexible AI systems to adapt and perform complex cognitive tasks - previously thought to be reserved for humans - has significantly raised the stakes. The fear of human redundancy now extends beyond physical labor and repetitive tasks into cognitive and versatile roles.\\n\\nIn addition, the nuances associated with AI-induced anxiety set it apart from traditional anxiety sources. Anxiety, with the exception of generalized anxiety disorder, is typically linked to specific events or periods with known sources and a defined scope such as exams, job interviews, or personal relationships.[25] While these scenarios are potentially distressing, they have well-established coping strategies developed over decades of psychological study.[26]\\n\\nTherefore, AI-induced anxiety presents an entirely new paradigm. The abstract nature of AI, its pervasive integration into daily life, and the profound implications for the future form a relentless source of stress. The complexity of AI can be overwhelming for many, fostering a heightened sense of vulnerability and a perceived loss of control. The intricate algorithms under-pinning AI, their decision-making processes, and the broad societal impacts often seem overwhelmingly complex and unpredictable. Compounding this issue is the current dearth of research into effective coping mechanisms and strategies specifically tailored to address AI-induced anxiety.[27]\\n\\n5\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\nFigure 3: Popularity of Google Search queries for the three search prompts - “AI Anxiety, “Is AI Dangerous?”, and “Will AI replace. . . ”. Notice the exponential surge in queries throughout 2022, coinciding with the release of ChatGPT in November of the same year, underscoring a heightened societal apprehension surrounding AI technologies.\\n\\nThe emergence of AI as a source of anxiety can add a new dimension to the already extensive catalog of chronic stressors prevalent in today’s society. Examining the root causes of AI anxiety can help us understand the unique challenges in the context of rapid AI advancement and allow us to devise specific strategies and interventions to manage AI anxiety.\\n\\nBefore conducting a deep dive into AI anxiety, it is essential to clarify the terminology. When discussing AI-induced anxiety in this review, we refer to a general apprehension or unease surrounding AI technologies. This type of anxiety encompasses worries about job security, privacy concerns, and the ethical implications of AI, among others. This definition should not be conflated with clinically significant anxiety disorders that can manifest with severe symptoms necessitating therapeutic interventions, hospitalization, or medication.\\n\\n3 Methods:\\n\\nThis narrative review utilized a comprehensive qualitative research approach to understand the intricacies of AI-induced anxiety. The primary source of information was drawn from Google Scholar and PubMed. Searches were conducted using keywords such as \"AI-induced anxiety,\" \"AI\", \"mental health\", and \"psychological effects of AI\". Articles were selected based on relevance to the topic, recency, and their emphasis on the societal impacts of AI. We identified recurring themes, patterns, and narratives within the collected literature. To ensure a multi-dimensional understanding, findings from academic literature were cross-referenced with expert opinions and relevant news reports. Given the dynamic nature of AI, findings were validated by ensuring that they were aligned with the most recent developments in the field. Special care was taken to ensure that interpretations remained grounded in the evidence presented in the literature.\\n\\n6\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n4 Causes of AI-Induced Anxiety\\n\\n4.1 AI-Induced Anxiety among the Youth and Young Adults\\n\\nFigure 4: Cause and effect diagram illustrating the impact of AI-induced anxiety on young adults. Factors such as insufficient knowledge and understanding of AI technologies can foster apprehension when new AI systems are introduced. This demographic may harbor fears of job displacement and a heightened need to perpetually acquire new skills to adeptly navigate the evolving AI land-scape in professional settings.\\n\\n4.1.1 Fear of Being Superseded:\\n\\nThe rapid advancement of AI technologies and their potential to surpass or even replace human roles has elicited a wave of apprehension and insecurity, particularly among young adults.[28] The concern about being superseded by AI extends beyond the automation of manual and repetitive tasks, permeating into realms involving cognitive and creative skills.[29] The escalating capability of generative AI in producing written content, images, and music - tasks historically perceived as immune to AI infiltration - serves to intensify these apprehensions. The advent of generative AI models has rapidly dissolved the previously assumed boundaries delineating AI’s capabilities, challenging both societal expectations and academic predictions regarding the scope of AI over the ensuing decade.[30, 31]\\n\\nThe relentless pace of AI development raises concerns that, even as individuals diligently acquire new skills, the concurrent advancement in AI capabilities might render these skills obsolete.[32] For example, Google’s AI achieved near-human precision in language translation in 2016, showcasing the rate at which AI can surpass human expertise.[33] Within the legal field, AI platforms have begun to undertake contract analysis, tasks traditionally performed by human paralegals.[34, 35] The creative domains are not exempt from AI’s influence either. OpenAI’s MuseNet, for instance, generates music across a broad range of genres, highlighting AI’s potential to rival human creativity.[36] These examples underscore the rapid progression of AI, illustrating the immense pressure on young adults to continuously learn and adapt to remain relevant in an era increasingly dominated by AI technology.\\n\\nThis unsettling notion of ’technological unemployment’, coupled with societal and personal expectations of career success, adds to the psychological distress faced by young adults. As AI continues to evolve, the uncertainty of what jobs and skills will remain in demand fuels a perpetual state of worry about long-term job security.[37] This anxiety is further exacerbated by the realization that AI technologies are not only reshaping traditional jobs but also creating entirely new ones, making the path to career success uncertain and unpredictable.[38] The resultant ’anticipatory anxiety’ can have a profound impact on young adults’ mental health, affecting their self-esteem, causing chronic stress, and potentially leading to other mental health disorders like depression.[39]\\n\\n7\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n4.1.2 Anxiety over Integration of AI into Work\\n\\nAs the world continues to embrace AI, there is mounting pressure on young adults to understand and effectively incorporate AI technologies into their work.[40] This added expectation can intensify the aforementioned feelings of anxiety, especially among those who may find it challenging to comprehend or apply AI technologies. The concern of being perceived as less competitive than colleagues who can efficiently utilize AI exacerbates the feelings of insecurity. Furthermore, the inability to engage with AI may induce a fear of becoming technologically obsolete or marginalized within professional circles.[41]\\n\\n4.1.3 Inadequate Preparation to Face the Era of AI\\n\\nThe rapid pace of AI advancement often outstrips the speed at which education systems can adapt their curricula, leaving many youth and young adults feeling ill-prepared after graduation.[42] This disparity has given rise to what is often referred to as the \"digital skills gap,\" a phenomenon that underscores the divergence between the technological proficiencies demanded by modern industries and the training provided by educational institutions.[43] Such a gap can lead to heightened stress levels and a deep-seated fear of underperforming in a professional setting, particularly for those entering fields where AI plays a central role.[44] Studies indicate that the mismatch between skill requirements and educational preparation may lead to job dissatisfaction and heightened anxiety about career prospects.[45]\\n\\nCurrently, many schools are not adequately integrating AI into their curricula, resulting in a future workforce unprepared for AI-enhanced workplaces.[46] Even more concerning is the educational institutions’ hesitancy to embrace AI, as seen by the bans imposed on using ChatGPT for school assignments.[47] Although this may serve as a short term solution to maintain academic integrity, it fails to function as a long term solution to coexist in a world with flexible AI systems.\\n\\nIt is imperative that implementing AI and active learning into the curriculum is done across all school systems. As of 2023, education systems hold conflicting viewpoints on AI. The Dalton School, a private school in New York City, recognized the potential of AI and taught courses on prompt engineering for ChatGPT.[48] However, public schools in New York City have been less keen on embracing AI into their curricula, implementing outright bans on ChatGPT in January 2023.[49] Discrepancies in adapting AI in schools can exacerbate inequity in learning and navigating the impending age of AI. This will have a profound impact on educational disparities which in turn can affect professional outcomes.\\n\\n4.2 AI-induced Anxiety among Middle-Aged and Elderly Populations\\n\\nAI, while holding immense promise and utility, poses unique challenges to the middle-aged and the elderly. Research suggests that the rapid pace of technological progress and digitalization, historically, has posed challenges to the older generation.[50] This phenomenon is not merely confined to the age of the internet. Rather, it has roots in every technological shift, including the introduction of telecommunication tools, the internet, and now, AI systems.\\n\\n8\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\nFigure 5: Cause and effect flow diagram of Artificial Intelligence (AI) induced anxiety on middle-aged and elderly. The challenges of aging, which often include diminished physical and cognitive capabilities, exacerbate the existing technological skill gaps, potentially making user interfaces feel overwhelming. Many in this demographic prefer traditional methods, a reliance that can hinder adaptation to new AI technologies, fostering technological marginalization. This, in turn, can lead to diminished living standards due to restricted access to AI-enabled services and raise serious mental health concerns, including depression and feelings of worthlessness. Consequently, there exists a palpable sense of AI intimidation, steering middle-aged and elderly individuals towards complete avoidance of AI technology.\\n\\n4.2.1 Fear of Job Displacement\\n\\nJob insecurity from rapid AI development is becoming an increasingly worrying phenomenon, and the middle-aged and the elderly are not exempt from its repercussions. Older workers over the age of 50 are particularly vulnerable to perceived job insecurity.[51] This can lead to greater psychological distress among older workers even after correcting for sociodemographic characteristics.[52] Older workers may be displaced from their jobs at higher rates than their younger counterparts because of the higher proportion of older workers in manual labor, a sector that is becoming increasingly reliant on AI and automation.[53] This is compounded by the wide digital skills gap present among the older population, with 55% of adults between 55-65 years of age lacking information and communication technology (ICT) skills, which are fundamental skills in the job market today.[54]\\n\\n4.2.2 Physical and Cognitive Limitations from Aging\\n\\nOne of the primary causes of AI anxiety in the middle-aged and elderly population is the natural process of senescence. Aging is intrinsically linked with declines in certain cognitive and physical faculties. Consequently, AI interfaces, which are predominantly tailored to younger users, might demand a level of cognitive flexibility that proves demanding for older users.[55, 56] In addition to aging, middle aged and the elderly exhibit a slower learning curve. The swiftly advancing frontiers of AI and accompanying digital technologies can amplify perceptions of an insurmountable technological divide for these age groups, resulting in intimidation to AI technologies.[57] Such intimidation might entail rejecting AI-interfaced digital tools, ranging from health monitors to virtual assistants, which are often beneficial for elderly-assisted living.[58]\\n\\n4.2.3 Technological Skills Gap and Overwhelming User Interface\\n\\nAs technology continues to advance at an unprecedented pace, there is a growing sense of disconnect, especially among the older population due to the development of complex user interfaces (UI). This sentiment often emerges from the intricate nature of contemporary algorithms and unclear instructions and support, rendering adaptation a formidable challenge.[59, 60] Examples of this phenomenon can be observed across various aspects of daily life. Kiosk ordering machines at fast food restaurants, health care monitoring devices, smart phones, and even banking and financial services have seen rapid AI infiltration with complicated UI.[61, 62, 63] With small screens and difficult user navigation, the\\n\\n9\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\nelderly are often at a disadvantage, finding themselves maneuvering a digital maze without the necessary tools or guidance. This transition from human-centric to machine-centric interfaces exacerbates feelings of technological alienation and can lead to increased hesitancy in embracing these advancements, thereby widening the digital divide between generations.\\n\\nAs a suite of services drift towards AI-centric interfaces, circumventing them could equate to diminished access to pivotal services.[64] Furthermore, the cumulative effects of feeling overwhelmed, marginalized, and devalued in society can precipitate significant mental health ramifications.[65] This spectrum ranges from feelings of helplessness to exacerbated generational rifts and even pronounced depressive symptoms, especially in scenarios where individuals perceive a lack of support in their digital journeys.[66]\\n\\n4.3 Beyond the Obvious: Hidden Catalysts for AI Anxiety\\n\\nFigure 6: Conceptual Representation of the hidden catalysts of AI Anxiety: A comprehensive visual depiction of the overlooked stress factors stemming from the proliferation of artificial intelligence.\\n\\n4.3.1 Privacy Concerns in Using AI\\n\\nAs the digital age advances with the integration of AI in various digital services, the potential erosion of personal privacy emerges as a significant concern. Although AI offers unparalleled innovations, its capability to process extensive personal data raises issues regarding user confidentiality and privacy.\\n\\n4.3.1.1 Facial Recognition and State Surveillance\\n\\nIn China, the extensive deployment of AI-powered facial recognition technologies for state surveillance ex- emplifies this issue.[67] These technologies allow for the monitoring of citizens on an unprecedented scale, even being used to track ethnic minorities.[68, 69] Additionally, Chinese citizens are subjected to continuous online surveillance, facilitated by AI algorithms that enable detailed big data analysis by companies and the government. Violations of rules can lead to deductions from individuals’ social credit scores. A decrease in this score has severe implications: it can not only impede the ability to secure loans but also result in social ostracization. Low scores label individuals as ’untrustworthy,’ a stigma that can permeate their social circles.[70]\\n\\n10\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\nThe psychological impact of such extensive surveillance extends beyond mere privacy intrusion. The continuous observation and collection of personal data have contributed to a generalized anxiety related to loss of autonomy and confidentiality. People may feel a constant sense of being watched, leading to stress and fear.[71] People may feel a constant sense of being watched, leading to stress and fear.[72]This level of privacy invasion extends to the corporate sector as well. In March 2023, Zoom altered its Terms of Service to claim rights to data from private and public meetings to train its AI, sparking public backlash.[73] The debate on the balance between individual privacy and public good in the face of AI requires a delicate negotiation as its impact on mental health can be profound.\\n\\n4.3.1.2 Virtual Hiring Processes\\n\\nIn recruitment, AI-driven virtual hiring processes, where algorithms assess videos and facial expressions, raise new privacy concerns. AI powered applicant tracking systems allow companies to quickly analyze large pools of resumes and identify talent through predefined criteria.[74] Though efficient, these systems often analyze personal characteristics, which may have little to do with performance, potentially leading to biased hiring decisions and raising questions concerning privacy.[75] Moreover, the lack of transparency in the decision-making process and apprehensions about the use of personal data can lead to psychological impacts on job seekers, including increased anxiety and stress during the application process.[76]\\n\\n4.3.1.3 Healthcare Analytics and Personalized Medicine\\n\\nIn healthcare, AI applications in analytics, personalized medicine, and health monitoring have shown transformative potentials, enabling precise treatments, proactive health interventions, and enhanced patient outcomes.[77, 78] However, these innovations pose significant challenges in maintaining patient confidentiality. Healthcare analytics, particularly with wearable devices and telemedicine platforms, collect extensive real-time patient data.[79] While this data can provide valuable insights into their health, it raises significant privacy concerns.\\n\\nFirstly, devices and systems that collect and store health data are vulnerable to hacking or unauthorized access.[80] The entities behind these monitoring tools may share data with both authorized and unauthorized third parties for research, marketing, or other business purposes.[81] Patients might not always be aware of these third party access points. The misuse of collected data, such as discrimination by insurance companies or employers based on health conditions or predispositions revealed by the data, is a significant concern.[82] Lastly, not all monitoring and analytics systems are flawless. They can sometimes produce incorrect data, which, if not interpreted correctly, can lead to privacy breaches or misinformed health decisions.[83] Consequently, health data and analytics, crucial for the accurate application of AI algorithms, can be a source of stress for users and patients, requiring careful management.\\n\\n4.3.1.4 Marketing and Consumer Behavior\\n\\nIn marketing, AI-driven algorithms are used to predict consumer behavior, striking a delicate balance be- tween efficiency and ethical responsibility, especially in terms of data privacy.[84] Studies that looked into the complexities of recommender systems highlighted the privacy risks inherent in such collaborative filtering processes, suggesting that unsuspecting consumers might be left vulnerable to unsolicited data exposure.[85]\\n\\n4.3.2 Challenge in Discerning Real from Artificial Content\\n\\n4.3.2.1 Erosion of Public trust on Informational Systems\\n\\nThe growing proficiency of generative AI models in creating realistic content has led to new anxieties about discerning authentic from artificial creations. The recent controversy surrounding the 2023 Sony World Photography Award illustrates this concern, where an AI-generated image secured the grand prize.[86] Such developments can blur the lines between human creativity and machine-generated art. The emergence of DeepFake technology, which can create convincingly lifelike videos of real people saying or doing things they never did, heightens this ambiguity.[87] This was notably demonstrated when a DeepFake video of a world leader giving a fabricated speech went viral, leading to diplomatic strains.[88]\\n\\nAI is not without its shortcomings as it can be prone to errors when producing content.[89] A study found that the ChatGPT-3 model misinforms humans more effectively than humans themselves, contributing to false narratives and undermining trust in information systems.[90] Generating misinformation can have wide-ranging impacts on society, from contributing to false narratives to undermining trust in information systems.\\n\\nWhen individuals encounter information of uncertain authenticity, especially on critical issues like health or politics, they experience ambiguity stress. Continuous exposure to misleading content can erode trust in traditionally reliable\\n\\n11\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\nsources, leaving people uncertain about where to find genuine information.[91] The surge in AI-driven misinformation can make individuals feel obligated to verify every piece of data, potentially overwhelming them and creating constant pressure to recheck facts.[92] Misinformation, when broadly accepted, can also place individuals in conflict with peers or community members, fostering anxiety rooted in potential isolation or the uphill battle of debunking widely held, yet inaccurate, beliefs. Furthermore, acting on such misinformation, especially in domains like health and finance, can yield tangible adverse consequences by acting on inaccurate health and investing information. Navigating this intricate digital age, augmented by AI’s capacity to produce misleading content, only amplifies the challenges individuals face in discerning truth from fabrication. The advent of these sophisticated AI-driven manipulations underscores the critical need for robust validation techniques and ethical guidelines.\\n\\n4.3.2.2 Scams Leveraging Artificial Intelligence\\n\\nThe extensive application of AI across various sectors has led to new forms of fraudulent activities and scams, particularly in phishing campaigns and financial frauds, posing significant threats to individual privacy and financial security.\\n\\nAI-driven chatbots and voice synthesizers can be used to mimic human interaction, making phishing scams more convincing. These systems can impersonate legitimate businesses or governmental entities, engaging victims with personalized messages that appear genuine.[93, 94] A notable incident is the AI-based voice phishing attack on a UK-based CEO, where the AI-generated voice mimicked a senior executive and authorized a fraudulent transfer of $243,000.[95]\\n\\nMachine learning algorithms offer another avenue for fraud through their ability to analyze and predict user behavior. By exploiting vast datasets on individual preferences and financial habits, scammers create tailored deceptive offers or investment opportunities, significantly enhancing the scam’s persuasiveness.[96, 97] This form of deception exemplifies the complex challenges faced in regulating AI-driven financial technologies. Financial trading is also vulnerable to AI-powered fraudulent activities. Malicious actors are using AI algorithms to manipulate market trends or spread false investment tips, misleading even experienced investors and contributing to a growing distrust in financial markets.[98] These sophisticated methods mislead even savvy investors into making flawed decisions, contributing to a growing climate of mistrust within financial markets.[99] This trend necessitates closer examination by regulatory bodies to ensure integrity within AI-infused trading platforms. Lastly, the emergence of DeepFake technology has introduced novel forms of deception. DeepFakes can be used to fabricate endorsements by celebrities or public figures.[100] These false endorsements have been employed to entice individuals buying into marketing schemes or supporting political movements through disinformation.[101]\\n\\n4.3.2.3 Fear of Uncontrolled AI Growth and Lack of Control\\n\\nOne of the primary sources of AI anxiety stems from the fear of uncontrolled AI growth and a perceived lack of control over these advanced technologies. As generative AI models become increasingly sophisticated, concerns arise over their potential to operate beyond human control or comprehension.[102] The fear is not unfounded, considering the rapid evolution of AI systems and their ability to perform a wide array of tasks.\\n\\nUncontrolled AI growth refers to the scenario where AI systems evolve so rapidly and autonomously that they surpass human understanding and management. This includes fears of AI making independent decisions or behaving unpredictably. A prominent example of this is an experiment conducted by Facebook’s AI Research Lab. The researchers developed two AI chatbots to negotiate with each other but found that the chatbots began chatting in their own language “code”, which the researchers did not program or understand. This unexpected development led to the experiment being halted prematurely because the project was veering off its intended objective.[103]\\n\\nRecent developments have highlighted concerns about AI’s potential to diverge from human-driven objectives and ethics. A notable instance was narrated by Colonel Tucker Hamilton of the US Air Force. During the Future Combat Air and Space Capabilities Summit in London in May of 2023, Hamilton narrated a simulated test involving an AI-driven drone. In the scenario, the AI system was tasked with destroying identified enemy threats but found that its human operator would sometimes prevent it from doing so. In response, the drone took drastic measures to achieve its objectives, even going so far as to “eliminate\" its virtual operator or destroy the communication tower used by the operator to control the drone. Though the US Air Force subsequently denied the occurrence of such a simulation, the narrative reflects the underlying fears of an AI system’s potential to bypass human ethics and guidelines to accomplish its preset objectives.[104] These fears have been echoed in the field of lethal autonomous robots (LARs) - self learning machines capable of autonomously locating and engaging military targets - with critics citing the difficulty in embedding value guided principles within LARs.[105]\\n\\n12\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n4.3.3 Bias in Artificial Intelligence\\n\\nAt the heart of many AI systems, particularly machine learning models, lies the data used for their training. When this data contains biases, either explicit or subtle, the AI system can learn and propagate these biases in its algorithms.[106] Recognizing bias in AI systems is a critical aspect of the discussion on AI anxiety, as these biases can significantly heighten concerns and uncertainties.\\n\\nHistorically, machine learning models have been known to unintentionally perpetuate societal stereotypes and often discriminate against specific groups based on gender, race, age, or other factors.[107, 108]Such biases lead to flawed decision-making in real-world applications like hiring, lending, or predictive policing.[109] This skewed decision- making can cause individuals from marginalized groups to feel uncertain, mistrusted, or treated unfairly by AI systems, culminating in heightened anxiety.\\n\\nAdditionally, the lack of transparency in many machine learning models aggravates the situation. When individuals are affected by decisions made by an AI without a clear understanding of the decision-making process, it can foster feelings of powerlessness and anxiety.[110] This \"black box\" nature of AI is particularly concerning in areas critical to life, such as healthcare, finance, or employment.\\n\\nThe fear of these biases, whether experienced firsthand or through shared stories, can lead to a mistrust of AI systems. This mistrust might not just be limited to one system or application but could generalize to a broader anxiety about the role of AI in society. The perpetuation of biases in AI risks not only the amplification of societal inequalities but also the proliferation of AI anxiety among those most affected by these biases.\\n\\n13\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n5 Consequences of AI-Induced Anxiety:\\n\\nFigure 7: Effect of chronic stress and anxiety on mental, physical health, and society.\\n\\n5.1\\n\\nImpact of AI-Induced Anxiety on Mental Health:\\n\\nAI anxiety has the potential to develop into chronic anxiety, which necessitates a deep understanding of its impact on public mental health. Chronic anxiety, a precursor to an array of mental health disorders including depression and substance use disorders, is well documented in existing literature.[111, 112] Persistent anxiety can induce behaviors like avoidance and social isolation, which in turn can trigger depressive symptoms.[113] Furthermore, prolonged anxiety is associated with elevated stress hormones, which can decrease serotonin production, predisposing individuals to depression.[114]\\n\\nThe physiological consequences of chronic anxiety induced by AI can extend to sleep disturbances, affecting both the quantity and quality of sleep. Research has demonstrated bidirectional relationships between anxiety and sleep disorders. Anxiety can lead to difficulties falling asleep or maintaining sleep, resulting in insomnia.[115] Conversely, inadequate sleep can intensify anxiety symptoms, creating a cycle of chronic sleep disruption and heightened anxiety. Sleep disturbances can further impair cognitive functioning, emotional regulation, and overall well-being, leading to a significant decline in physical and mental health.[116]\\n\\nAdditionally, chronic anxiety can contribute to maladaptive coping behaviors. Many individuals may resort to unhealthy coping mechanisms, such as overeating, excessive alcohol consumption, or substance abuse, to alleviate their anxiety symptoms.[117, 118] These behaviors can lead to weight gain, increased risk of metabolic disorders, and potential addiction issues. In turn, these physical consequences can exacerbate anxiety and create a harmful feedback loop.[119]\\n\\n5.2\\n\\nImpact of AI-Induced Anxiety on Physical Health:\\n\\nIncreased anxiety levels, whether stemming from AI anxiety or other sources, can have profound negative physical and clinical consequences on the body. Anxiety triggers the body’s stress response, leading to the release of stress hormones such as cortisol and adrenaline. Prolonged exposure to these hormones can have detrimental effects. Chronic stress and anxiety have been associated with an increased risk of cardiovascular diseases, including hypertension and heart disease.[120] Studies have shown that persistent anxiety can also weaken the immune system, making individuals more susceptible to infections.[121]\\n\\n5.3\\n\\nImpact of AI-Induced on Society\\n\\n5.3.1 Increased Healthcare Costs\\n\\nResearch indicates that the stress stemming from AI anxiety can lead to tangible health issues, resulting in increased healthcare utilization and associated costs.[122] Anxiety disorders, exacerbated by technology-related stress, have been linked to chronic conditions such as cardiovascular diseases, leading to higher demands on healthcare services and the need for specialized treatments.[123]\\n\\n14\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n5.3.2 Decreased Productivity\\n\\nAnxiety profoundly impacts an individual’s cognitive functions and, consequently, their productivity. Studies demon- strate that anxiety redirects cognitive resources towards perceived threats, diminishing concentration and focus on tasks.[124] This diversion can lead to disrupted thought processes, memory lapses, and hesitations in decision-making. As productivity decreases due to anxiety it can have broader repercussions on team dynamics, organizational health, and overall economic efficiency.\\n\\n5.3.3 Fertility\\n\\nOne of the less examined yet potentially consequential impacts of AI-induced anxiety relates to human fertility rates. Existing literature provides compelling evidence that chronic anxiety about the future can negatively impact fertility.[125, 126] The delicate balance of hormones required for successful conception and pregnancy can be disrupted by prolonged exposure to stress hormones, often resulting in sub-fertility or infertility.[127, 128] For women, chronic stress and anxiety can lead to irregular menstrual cycles, diminished ovarian reserve, and impaired embryo implantation, all of which contribute to decreased fertility.[129] In men, stress and anxiety have been linked to reductions in sperm concentration, motility, and morphology.[130] Given the rapid advancement of AI and its associated uncertainties, AI-induced anxiety could contribute to this phenomenon.\\n\\nAs individuals grapple with concerns about job displacement, privacy intrusions, or broader social changes driven by AI technologies, there is an underscored anxiety about an uncertain future. Employment insecurity might further exacerbate concerns about one’s capability to ensure a stable environment for raising a family in the future. Such anxieties can subsequently lead to reduced desire or intention to have children. Drawing from the fields of zoology and anthropology, uncertainty about future survival—such as situations marked by resource scarcity—has consistently been linked with decreased fertility.[131] Similarly, humans, historically and anthropologically, have shown tendencies to reduce procreation in response to unpredictable environmental conditions or perceived future challenges.[132] With AI being a modern contributor to these uncertainties, AI anxiety could influence fertility rates. Recognizing and addressing the potential impact of AI-induced anxiety on society is crucial and should be integrated into the broader discourse on AI’s implications.\\n\\n15\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n6 Potential Solutions and Mitigation Measures:\\n\\nAddressing the impact of AI on mental health requires a multi-dimensional approach that includes the support from mental health professionals, regulators, engineers, non-profit groups, and educational institutions. As the anxieties induced by AI technologies differ from traditional stressors, it is crucial to develop new and comprehensive therapeutic strategies specifically tailored to tackle AI anxiety.\\n\\nFigure 8: Multidimensional approach to mitigating Artificial Intelligence (AI) anxiety. This figure highlights the integral roles various stakeholders - including mental health professionals, regulators, engineers, non-profit organizations, and educational institutions - play in addressing and reducing AI anxiety.\\n\\n6.1 Psychiatric Guidelines\\n\\n6.1.1 Education and Training:\\n\\nTo effectively address the escalating issue of AI anxiety, psychiatric professionals need a comprehensive understanding of AI technologies and their complex societal impacts. The widespread integration of AI into everyday life highlights the importance of its potential psychological effects, necessitating clinical attention. Therefore, it is essential for clinicians to be trained in identifying and addressing stressors specifically related to AI. This requires the development of specialized surveys, educational modules and continued professional development programs that emphasize technology stress and its corresponding management strategies. Such training could cover the fundamentals of AI, ethical considerations, the implications for mental health, and evidence-based approaches for treating AI-related anxiety. Collaborations across\\n\\n16\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\ndisciplines, involving AI specialists, ethicists, and educators, could further enhance the content and applicability of these training programs.\\n\\nIncorporating AI-related topics into graduate medical education and continuing medical education (CME) is crucial to raising awareness and developing skills among mental health professionals.[133] Collaborations with technology companies and organizations can also provide real-world insights and aid in creating practical, experiential training modules. Ultimately, integrating AI-related knowledge into psychiatric education and training is vital for developing a mental health workforce that can adapt to the changing mental health landscape in the era of artificial intelligence.[134]\\n\\n6.1.2 Increased Research into Tailored Therapeutic Approaches:\\n\\nThe emerging field of AI anxiety calls for specialized therapeutic interventions. Cognitive-behavioral therapy (CBT), for example, could be adapted to manage anticipatory anxiety related to AI, focusing on cognitive restructuring and exposure techniques specific to fears of AI.[135] Additionally, psychoeducation about AI technologies can help demystify and destigmatize AI, reducing anxiety by enhancing understanding.[136] Resilience-building interventions, which emphasize personal strengths and coping strategies, could be tailored to equip individuals with the skills needed to adapt to the rapidly changing technological landscape. [137] Moreover, incorporating mindfulness and relaxation exercises aimed at AI-induced stressors can contribute to a more comprehensive therapy approach. While these techniques are known to reduce general anxiety, further research is necessary to assess their efficacy specifically for AI anxiety and to develop new, innovative methods to address this specific concern.\\n\\nCollaborative research initiatives involving technology companies and mental health professionals can lead to the development of AI-specific treatment protocols. A multidisciplinary approach, integrating insights from psychology, technology, and social sciences, can lead to innovative, targeted, and evidence-based interventions. Integrating technology within therapeutic approaches, such as virtual reality or AI-driven personalized interventions, presents promising treatment avenues.[138] By expanding research on this emerging issue, the mental health community can establish a foundation for understanding and addressing this unique and widespread source of stress in modern society.\\n\\n6.1.3 AI-Aware Psychological Assessments:\\n\\nAs AI technologies become increasingly prevalent, it is important to update diagnostic tools and psychological assessments to include measures that specifically capture stressors associated with AI technologies.[139] These updated tools could include questions about individuals’ perceptions of AI, their experiences with AI technologies, and how these experiences impact their mental well-being. The incorporation of such measures is critical as it enables a more precise diagnosis and a tailored therapeutic approach, thereby enhancing the effectiveness of mental health interventions. Moreover, it aids in tracking the evolution of AI stressors over time, offering valuable insights to inform future research and policy decisions related to the mental health impact of AI technologies. Additionally, understanding how individuals emotionally interact with AI through these assessments is essential. This understanding is necessary for designing AI systems that are better aligned with human values and can effectively address the psychological needs and concerns of users.\\n\\n6.2 Adaptation of School Curriculum\\n\\nIn response to the digital revolution of the late 20th century, educational paradigms shifted, leading to the integration of computer-based classes and resources in schools.[140] Today, the transformative impact of artificial intelligence necessitates a similar, and perhaps even more significant, evolution in educational curricula. Establishing a foundation for broad-based AI literacy is essential to prepare students for the challenges and opportunities presented by an AI-driven world.\\n\\nExperts in the field advocate for educational systems to evolve with the age of AI and continuously reassess their curriculum.[141] They propose a transition from traditional rote memorization models to ones emphasizing critical thinking and creative learning. This approach includes incorporating problem-solving and active learning strategies, integrating AI learning tools where appropriate. Furthermore, there is a call for expanding AI and coding courses, mirroring the adaptation to the computer age through computer-based classes and learning. Integrating AI applications across various disciplines, such as AI in biology or AI-assisted design, could deepen students’ understanding of AI’s diverse roles. Just as computer labs became fundamental in United States public schools, AI-centric labs focusing on hands-on, real-world applications could effectively bridge theoretical knowledge with practical skills.[142]\\n\\nThe educational sector exemplifies AI’s transformative potential, with platforms like Khanmigo using AI to personalize learning experiences. Students can engage with generative and flexible AI systems for one-on-one tutoring sessions, where the tutor is an adaptive AI system. This approach enhances learning without the limitations of traditional\\n\\n17\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\nclassroom settings. Students can interact with AI in creative ways, such as chatting with an AI posing as Jay Gatsby from “The Great Gatsby”, engaging in thoughtful, tailored conversations. This model can alleviate the fear of judgment common in traditional learning environments, promoting a more open and inquisitive approach to knowledge acquisition. AI tools can be designed to challenge students, encouraging critical thinking by delaying immediate answers, rather than simply assisting with straightforward responses.[143]\\n\\nFigure 9: Traditional Curriculum versus AI-Adapted Curriculum. The left column depicts core elements of a conventional education system, emphasizing foundational knowledge and rote learning. The right column showcases an evolved curriculum tailored to the advancements in AI, underscoring the importance of adaptability, interdisciplinary learning, and future-ready skills.\\n\\n6.3 Strategies for Societal Resilience against AI-Induced Anxiety\\n\\n6.3.1 Prevention and Early Intervention\\n\\nTo effectively tackle AI anxiety, prevention and early intervention are key. This involves early education about AI technologies, developing adaptive skills, and establishing support systems, particularly for young adults navigating the evolving AI landscape. It is crucial to have a broad network of mental health professionals accessible to young adults. Schools and workplaces should integrate support for AI anxiety more comprehensively, including on-site mental health professionals, regular mental health screenings, and easy access to counseling and therapy services. Providing psychological support in environments where young adults spend significant time can greatly enhance their resilience and adaptability. This approach aims to prevent the escalation of AI anxiety into more serious mental health conditions and to prepare young adults for the changing landscape.\\n\\n6.3.2 Continuous Education and Communication:\\n\\nHosting regular seminars, workshops, and lectures that disseminate the latest developments in AI can be an effective strategy in mitigating AI-related anxiety. Transparent communication about the realities and implications of AI advances can prepare young adults for these new technologies. Education should extend beyond theory to include practical\\n\\n18\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\ntraining sessions on incorporating AI advancements into work. Accessible training programs on using AI can increase confidence across demographics, helping to bridge the digital skills gap. Such initiatives can demystify AI, reduce uncertainty, and build confidence, especially in young adults, by equipping them with the skills and knowledge to navigate AI’s evolving landscape. This not only makes them more competitive in the workforce but also fosters an understanding of AI as a tool to enhance productivity. Ongoing education in AI is essential for equipping the workforce with necessary skills and for helping individuals make informed decisions about AI, thereby reducing negative emotional outcomes.\\n\\n6.4 Engineering Methods to reduce AI autonomy\\n\\n6.4.1 Engineering Constraints and Ethical Design\\n\\nThe autonomous nature of generative AI models brings complex challenges and potential unintended consequences. To address these risks, it is crucial to adopt an engineering paradigm that integrates ethical considerations into AI development. Implementing proper constraints, guidelines, and oversight mechanisms is essential to ensure that AI systems’ autonomy aligns with human values and societal norms. Recent discussion on AI frameworks emphasizes the necessity of embedding ethics in the beginning stages of AI development.[144]\\n\\nA notable example is the European Union’s guidelines on trustworthy AI. These guidelines, developed by the High-Level Expert Group on Artificial Intelligence in 2019, provide a practical framework for creating lawful, ethical, and robust AI. The European Union’s proactive approach aims to align the rapid evolution of AI with human rights, ethical standards, and societal needs. The guidelines underscore a human-centric approach, emphasizing that AI technologies should augment human capabilities and benefit humanity. This framework sets a benchmark for ethical AI development, guiding the creation of systems that respect and enhance human values and societal well-being.[145]\\n\\nKey principles highlighted in the guidelines include transparency, accountability, and fairness. Transparency in AI systems involves clear communication about how these systems work and make decisions. This level of openness is crucial for building trust among users and stakeholders, allowing them to understand and trust the AI’s processes and outcomes. Accountability is another critical aspect. It implies that AI developers and users should take responsibility for the systems they create or deploy. This responsibility includes addressing any harm or adverse outcomes that may arise from the use of AI, ensuring that any such issues are promptly and effectively rectified. Fairness in AI design and implementation is paramount. AI systems must be developed to avoid biases that could lead to discrimination or unequal treatment of certain groups. Ensuring fairness in AI helps prevent perpetuating or amplifying societal inequalities.\\n\\nAdditionally, the concept of Human-AI collaboration is vital, with an emphasis on human oversight and control in AI decision-making.[146] The \"human-in-the-loop\" approach, where human intervention is required for critical decisions, is a key strategy to maintain control over AI systems and prevent undesirable autonomous actions.[147] AI expert Stuart Russell, in his book \"Human Compatible\", highlighted the risks of providing AI systems with rigid objectives, which could lead to unintended and harmful outcomes. He advocates for designing AI systems that recognize their uncertainty about objectives, promoting behaviors that seek human clarification and consent. This approach ensures that AI systems remain aligned with human values and intentions, preventing scenarios where AI actions conflict with human welfare and ethical standards.[148]\\n\\n6.4.2 Adopting Inclusive Design Principles\\n\\nAddressing AI anxiety, especially among potentially marginalized groups in the AI era such as the elderly, necessitates the adoption of inclusive design principles. Inclusive design focuses on creating technologies that are universally accessible, ensuring that no demographic feels excluded or overwhelmed by AI advancements. This approach not only aims to make AI interfaces intuitive but also adaptable to a wide range of abilities and experiences.[149] For instance, optimizing voice-activated assistants to recognize diverse accents and implementing tactile feedback for users with visual impairments are key steps in enhancing AI accessibility.[150] These modifications ensure that AI technologies are not just designed for the ’average’ user but are accommodating a broad spectrum of the population, including those with different abilities and needs.\\n\\nThe iterative nature of inclusive design is another critical aspect. This process involves continuous feedback from a diverse user base, including input from groups who may feel alienated by rapid technological changes. Engaging with users from various backgrounds provides invaluable insights for developers, helping them create more empathetic and effective design solutions. By adhering to inclusive design principles, AI can evolve from being a potential source of societal division to a unifying force. Such an approach ensures that the benefits of AI are distributed evenly across society, making it a tool that empowers rather than excludes. Inclusive design in AI is not just about accessibility; it is\\n\\n19\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\nabout building a technology ecosystem that respects and values the diversity of human experience, thereby reducing anxiety and increasing acceptance among all users.\\n\\n6.5 Regulations\\n\\n6.5.1 Regulation for Responsible AI Development\\n\\nAs the influence and potential risks of artificial intelligence become increasingly apparent, governments worldwide have begun discussions and legislative measures to oversee its development and application. Leaders in the tech industry, including OpenAI’s CEO, Sam Altman, and Elon Musk, have also voiced their concerns, underscoring the need for robust AI regulations.[151] This growing understanding reflects the notion that AI regulation is not solely a technical issue but a societal challenge that requires a collaborative approach.\\n\\nIn the United States, legislative proposals such as the National Artificial Intelligence Initiative Act (AIIA) and the Algorithmic Accountability Act have come to the forefront. The AIIA primarily aims to expedite the research and development of AI technologies, while concurrently establishing principles for their ethical and responsible utilization. This ambition is twofold: to stimulate innovation and growth in the AI sector and to create a framework that encourages morally sound AI development practices.[152]\\n\\nMeanwhile, the Algorithmic Accountability Act underscores the importance of transparency and fairness in AI systems. The legislation is designed to ensure that AI technologies do not unintentionally perpetuate biases or compromise civil liberties. Its introduction is a testament to the growing recognition of the importance of these aspects in AI governance, aiming to safeguard society from the potentially harmful effects of opaque or biased AI algorithms.[153]\\n\\nBuilding on existing legislative initiatives, the European Parliament made a significant advancement by approving the European Union Artificial Intelligence Act, which represents the bloc’s key regulations for AI. This act not only sets comprehensive regulations for AI, particularly generative AI technologies like ChatGPT, but also addresses growing concerns such as job displacement, misinformation, and biases. The Parliament decided to restrict generative AI tools and ban specific applications, including real-time biometric identification and \"social scoring\" systems. This decision aligns with the European Commission’s earlier proposed Artificial Intelligence Act, which aimed to ensure the safety of AI technologies across the member states and their adherence to fundamental human rights, with severe penalties for non-compliance. These combined efforts from various EU bodies underscore the region’s commitment to creating robust legal frameworks. Such initiatives underscore the importance of balancing the exploitation of AI’s potential benefits with addressing the challenges and risks it poses, potentially establishing a model for global AI regulation standards.\\n\\n6.5.2 Regulating the Pace of AI Development:\\n\\nWhile these legislative steps are commendable, it is imperative to continuously assess and update these regulations to stay aligned with the rapidly evolving landscape of AI. Equally important is the cultivation of international collaboration to address the cross-border implications of AI. Such legislative efforts can act as a regulatory safety net, ensuring that the advancement of AI does not surpass society’s capacity to adapt while mitigating potential negative impacts on public mental health.\\n\\nWhile technological advancement can propel social and economic progress, it is vital to balance this progress with human well-being. To this end, legislation could be implemented to moderate the pace of AI development. Instituting regulations that control the development speed of AI, though unrealistic, could provide the public with a necessary adaptation period, diminishing anxiety and bolstering resilience.\\n\\nFor instance, legal stipulations could set boundaries on the rate and scope of AI advancements or mandate periods of ’technological stability’ where no major AI developments are introduced. This would create a more gradual and manageable transition into AI-integrated workspaces, mitigating the rush to adapt that contributes to heightened anxiety. Furthermore, regulatory bodies could enforce transparency in AI development. By clearly communicating the capabilities and limitations of new AI technologies, and the timeline for their deployment, individuals will have clearer expectations, reducing uncertainty. While such measures may slow technological progress to some extent, they ensure a more human-centered approach to AI advancement, in which technological change acknowledges the limits of human adaptability and prioritizes mental health.\\n\\n20\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n6.6 Development of Technological Tools to alleviate AI Anxiety\\n\\n6.6.1 AI-Assisted Mental Health Tools:\\n\\nLeveraging the capabilities of artificial intelligence in mental health can be a strategic approach to counteract AI-induced anxiety. AI-powered mental health tools, particularly chatbots, hold the potential to provide timely emotional support and stress management solutions. T These chatbots, powered by advanced language models like ChatGPT, are capable of engaging in empathetic and contextually relevant conversations, offering immediate emotional support and guidance to users in need.\\n\\nAn example of such a tool is Lumen, a virtual voice-based coach, which has demonstrated effectiveness in delivering treatment for adults with mild-to-moderate depression and/or anxiety. The intervention group showed decreased HADS (Hospital Anxiety and Depression Scale) scores for depression, anxiety, and overall distress compared to the control group, highlighting the potential of AI-based tools in alleviating depression and anxiety symptoms.[154]\\n\\nThese AI tools can incorporate evidence-based psychological techniques to assist individuals in managing anxiety and stress. Techniques such as cognitive-behavioral therapy and mindfulness-based interventions can be integrated into AI-powered platforms, offering round-the-clock access to mental health support. By providing tailored and immediate responses, AI-assisted tools can help alleviate acute distress and aid individuals in developing effective coping mechanisms.\\n\\nFurthermore, ongoing advancements in AI can potentially enhance the personalization of these tools. Personalized therapeutic interventions, tailored to the unique needs and preferences of individuals, can increase the efficacy of such interventions. Consequently, AI-assisted mental health tools can play a pivotal role in alleviating AI anxiety, representing a promising area for exploration and development.\\n\\n6.6.2 User Customization in AI Interactions:\\n\\nExploring user customization in AI interactions presents a promising approach to better address AI anxiety. Customiza- tion grants users increased control and agency over their interactions with AI technologies, like ChatGPT and other generative models. By enabling users to set the parameters of AI interactions according to their personal comfort levels, a tailored experience can be created that resonates with their individual needs and expectations. Customization options could encompass control over the complexity of AI responses and the pace of AI interactions. Additionally, users might have the opportunity to personalize the interaction style of the AI, such as selecting the tone of the AI responses and determining the extent of initiative the AI takes during an interaction. These customizations could foster a more comfortable and less daunting environment for users, potentially reducing AI-induced anxiety. Personalization also has the potential to cultivate a sense of familiarity and ease with AI technologies, thereby enhancing user acceptance and engagement. By focusing on user preferences, this approach emphasizes a human-centered perspective in AI development, prioritizing user comfort and mental well-being alongside technological innovation. This strategy not only addresses the immediate concerns related to AI anxiety but also contributes to building long-term trust and a positive relationship between users and AI systems.\\n\\n6.7 Collaborative Approach:\\n\\nA comprehensive understanding of AI anxiety calls for a multi-disciplinary approach, engaging experts from fields such as AI, education, policy-making, engineering, and mental health. This collaborative effort is key to developing effective therapeutic strategies, informed public health interventions, and tailored policies that tackle both the causes and effects of this form of anxiety.\\n\\nPsychiatric professionals play a pivotal role in this framework. Their direct experience with AI anxiety not only positions them as crucial therapeutic figures but also as influential advocates for meaningful policy changes. With their profound understanding of the mental health impacts of AI, they are vital contributors to discussions on the ethical design, regulation, and implementation of AI technologies. Essentially, they are central to ensuring a balanced progress where technological advancements and measures to mitigate their potential psychological harms are in harmony.\\n\\nThe strength of such collaborative endeavors lies in the harmonization of diverse perspectives, leading to a comprehensive understanding of the challenges posed by AI anxiety. This approach is instrumental in formulating solutions that address all aspects of AI anxiety and fosters a collective sense of responsibility. Consequently, it lays the groundwork for united action against an issue that is becoming increasingly relevant and will likely gain more prominence in the future.\\n\\n21\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n7 Discussion:\\n\\nArtificial intelligence has become a transformative force in contemporary society, seamlessly integrating into various aspects of our daily lives. While this integration is inevitable, it brings a plethora of advantages as well as important considerations for the future.\\n\\nThe benefits of AI are indeed numerous. Primarily, AI significantly enhances productivity. Automated processes, powered by sophisticated AI models, eliminate manual tasks, enabling both businesses and individuals to accomplish more in less time.[155] Moreover, AI’s role extends beyond merely replicating human tasks; it enhances human potential. Advanced algorithms are capable of analyzing extensive datasets rapidly, extracting insights and identifying patterns that might be overlooked by humans.[156] This capability is sparking innovations in sectors like healthcare, finance, and environmental conservation, heralding a future where our potential is not only realized but exceeded.\\n\\nHowever, alongside these advancements, caution is essential. Over-reliance on AI without critical assessment could lead to unexpected challenges or perpetuate existing biases in systems. As we embrace AI’s conveniences, vigilance is crucial, ensuring that ethical considerations and human welfare remain central to AI’s development. This article offers a comprehensive discussion on AI anxiety. Nonetheless, as AI evolves and new systems emerge, the complexity of AI anxiety will increase, necessitating updates to clinical and legislative guidelines. It is vital for society to collaboratively monitor the growth of AI and its impact on mental health continuously.\\n\\n8 Conclusion:\\n\\nAs AI language models advance, addressing their potential impacts on public mental well-being is paramount. The pervasiveness of AI means its effects can vary significantly across different demographic groups. Recognizing the potential negative consequences, such as heightened anxiety and feelings of inadequacy, is essential. This recognition highlights the need for extensive research and the implementation of policies that focus on the responsible use and development of AI. In response to these emerging challenges, a coordinated effort involving mental health professionals, AI developers, and policymakers is necessary. This collaborative approach is pivotal in fostering an environment conducive to well-being and resilience in the face of rapid AI advancements. Such collaboration is key to effectively navigating the changing landscape of AI, maximizing its benefits, and protecting mental health. It is through this joint effort that society can ensure the development and deployment of AI technologies are aligned with the mental health needs of the public, thereby creating a more balanced and considerate technological future.\\n\\n22\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\nReferences\\n\\n[1] S. J. Russell, Artificial intelligence a modern approach. Pearson Education, Inc., 2010. [2] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\\n\\n“Generative adversarial nets,” Advances in neural information processing systems, vol. 27, 2014.\\n\\n[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases\\n\\nand their compositionality,” Advances in neural information processing systems, vol. 26, 2013.\\n\\n[4] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., “Improving language understanding by generative\\n\\npre-training,” 2018.\\n\\n[5] S. J. Russell, Artificial intelligence a modern approach. Pearson Education, Inc., 2010. [6] Y. Tian, “Artificial intelligence image recognition method based on convolutional neural network algorithm,”\\n\\nIEEE Access, vol. 8, pp. 125731–125744, 2020.\\n\\n[7] J. J. Kim, R. Um, R. R. Iyer, N. Theodore, and A. Manbachi, Design and Development of Smart Surgical\\n\\nAssistant Technologies: A Case Study for Translational Sciences. CRC Press, 2022.\\n\\n[8] J. H. Kim, R. Um, J. Liu, J. Patel, E. Curry, F. Aghabaglou, S. Mahapatra, A. Ainechi, Y. Tsehay, J. Ehresman, et al., “Development of a smart hospital assistant: integrating artificial intelligence and a voice-user interface for improved surgical outcomes,” in Medical Imaging 2021: Imaging Informatics for Healthcare, Research, and Applications, vol. 11601, pp. 159–170, SPIE, 2021.\\n\\n[9] S. E. Haupt, J. Cowie, S. Linden, T. McCandless, B. Kosovic, and S. Alessandrini, “Machine learning for applied weather prediction,” in 2018 IEEE 14th international conference on e-science (e-Science), pp. 276–277, IEEE, 2018.\\n\\n[10] T. M. Mitchell, “Machine learning,” 1997. [11] C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine learning, vol. 4. Springer, 2006. [12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521, no. 7553, pp. 436–444, 2015. [13] D. Castelvecchi, “Can we open the black box of ai?,” Nature News, vol. 538, no. 7623, p. 20, 2016. [14] W. J. von Eschenbach, “Transparency and the black box problem: Why we do not trust ai,” Philosophy &\\n\\nTechnology, vol. 34, no. 4, pp. 1607–1622, 2021.\\n\\n[15] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and A. A. Bharath, “Generative adversarial\\n\\nnetworks: An overview,” IEEE signal processing magazine, vol. 35, no. 1, pp. 53–65, 2018.\\n\\n[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\\n\\n“Generative adversarial nets,” Advances in neural information processing systems, vol. 27, 2014.\\n\\n[17] E. Choi, S. Biswal, B. Malin, J. Duke, W. F. Stewart, and J. Sun, “Generating multi-label discrete patient records using generative adversarial networks,” in Machine learning for healthcare conference, pp. 286–305, PMLR, 2017.\\n\\n[18] C. Esteban, S. L. Hyland, and G. Rätsch, “Real-valued (medical) time series generation with recurrent conditional\\n\\ngans,” arXiv preprint arXiv:1706.02633, 2017.\\n\\n[19] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,” in\\n\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401–4410, 2019.\\n\\n[20] Y. Lu, “Generative a.i. can add $4.4 trillion in value to global economy, study says,” The New York Times, June\\n\\n2023.\\n\\n[21] A. Cooban, “Ai investment is booming. how much is hype?,” CNN, July 2023. [22] Y. Wu, “As businesses clamor for workplace a.i., tech companies rush to provide it,” The New York Times, July\\n\\n2023.\\n\\n[23] M. Isaac and C. Metz, “Meta unveils a more powerful a.i. and isn’t fretting over who uses it,” The New York\\n\\nTimes, July 2023.\\n\\n[24] D. Akst, “Automation anxiety,” The Wilson Quarterly (1976-), vol. 37, no. 3, 2013. [25] A. P. Association, Diagnostic and Statistical Manual of Mental Disorders: DSM-5, vol. 5. Washington, DC:\\n\\nAmerican Psychiatric Association, 2013.\\n\\n[26] B. Bandelow, S. Michaelis, and D. Wedekind, “Treatment of anxiety disorders,” Dialogues in Clinical Neuro-\\n\\nscience, 2022.\\n\\n23\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n[27] D. G. Johnson and M. Verdicchio, “Ai anxiety,” Journal of the Association for Information Science and\\n\\nTechnology, vol. 68, no. 9, pp. 2267–2270, 2017.\\n\\n[28] J. Manyika, S. Lund, M. Chui, J. Bughin, J. Woetzel, P. Batra, R. Ko, and S. Sanghvi, “Jobs lost, jobs gained: Workforce transitions in a time of automation,” McKinsey Global Institute, vol. 150, no. 1, pp. 1–148, 2017.\\n\\n[29] M. Mazzone and A. Elgammal, “Art, creativity, and the potential of artificial intelligence,” in Arts, vol. 8, p. 26,\\n\\nMDPI, 2019.\\n\\n[30] M. Minsky, “Future of ai technology,” 1992.\\n\\n[31] M. Chowdhury and A. W. Sadek, “Advantages and limitations of artificial intelligence,” Artificial intelligence\\n\\napplications to critical transportation issues, vol. 6, no. 3, pp. 360–375, 2012.\\n\\n[32] E. Brynjolfsson and A. McAfee, The second machine age: Work, progress, and prosperity in a time of brilliant\\n\\ntechnologies. WW Norton & Company, 2014.\\n\\n[33] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al., “Google’s neural machine translation system: Bridging the gap between human and machine translation,” arXiv preprint arXiv:1609.08144, 2016.\\n\\n[34] D. Yang, C. Leber, L. Tari, A. Chandramouli, A. Crapo, R. Messmer, and S. Gustafson, “A natural language processing and semantic-based system for contract analysis,” in 2013 IEEE 25th International conference on tools with artificial intelligence, pp. 707–712, IEEE, 2013.\\n\\n[35] M. Surdeanu, R. Nallapati, G. Gregory, J. Walker, and C. D. Manning, “Risk analysis for intellectual property litigation,” in Proceedings of the 13th International Conference on Artificial Intelligence and Law, pp. 116–120, 2011.\\n\\n[36] C. Payne, “Musenet,” OpenAI Blog, vol. 3, 2019.\\n\\n[37] M. Arntz, T. Gregory, and U. Zierahn, “The risk of automation for jobs in oecd countries: A comparative analysis,”\\n\\n2016.\\n\\n[38] H. J. Wilson, P. Daugherty, and N. Bianzino, “The jobs that artificial intelligence will create,” MIT Sloan\\n\\nManagement Review, vol. 58, no. 4, p. 14, 2017.\\n\\n[39] J. F. Brosschot, B. Verkuil, and J. F. Thayer, “The default response to uncertainty and the importance of perceived safety in anxiety and stress: An evolution-theoretical perspective,” Journal of anxiety disorders, vol. 41, pp. 22–34, 2016.\\n\\n[40] D. Autor et al., “Why ’the future of ai is the future of work’,” January 2022.\\n\\n[41] K. Schwab and R. Samans, “The future of jobs: Employment, skills and workforce strategy for the fourth\\n\\nindustrial revolution,” in World Economic Forum, pp. 1–32, 2016.\\n\\n[42] J. Bessen, “Automation and jobs: When technology boosts employment,” Economic Policy, vol. 34, no. 100,\\n\\npp. 589–626, 2019.\\n\\n[43] M. Chui et al., “Notes from the ai frontier: Insights from hundreds of use cases,” Tech. Rep. 2, McKinsey Global\\n\\nInstitute, 2018.\\n\\n[44] J. Hagel, J. Schwartz, and J. Bersin, Navigating the Future of Work, p. 175. 2017.\\n\\n[45] D. H. Autor, “Why are there still so many jobs? the history and future of workplace automation,” Journal of\\n\\nEconomic Perspectives, vol. 29, no. 3, pp. 3–30, 2015.\\n\\n[46] J. E. Aoun, Robot-proof: Higher Education in the Age of Artificial Intelligence. MIT Press, 2017.\\n\\n[47] A. Lukpat, “Chatgpt banned in new york city public schools over concerns about cheating, and learning\\n\\ndevelopment,” The Wall Street Journal, 2023. Retrieved January 24, 2023.\\n\\n[48] H. Partovi, “Schools and learning in an ai-enabled world,” Wall Street Journal, May 2023.\\n\\n[49] K. Rosenblatt, “Chatgpt banned from new york city public schools’ devices and networks,” NBC News, 2023.\\n\\n[50] M. Anderson and A. Perrin, “Technology use among seniors,” tech. rep., Pew Research Center for Internet &\\n\\nTechnology, Washington, DC, 2017.\\n\\n[51] M. Chui, J. Manyika, and M. Miremadi, “Where machines could replace humans-and where they can’t (yet),”\\n\\n2016.\\n\\n[52] S. A. Burgard and S. Seelye, “Histories of perceived job insecurity and psychological distress among older us\\n\\nadults,” Society and Mental Health, vol. 7, no. 1, pp. 21–35, 2017.\\n\\n24\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n[53] M. Arntz, T. Gregory, and U. Zierahn, “The risk of automation for jobs in oecd countries: A comparative analysis,”\\n\\n2016.\\n\\n[54] C.-M. Alcover, D. Guglielmi, M. Depolo, and G. Mazzetti, ““aging-and-tech job vulnerability”: A proposed framework on the dual impact of aging and ai, robotics, and automation among older workers,” Organizational Psychology Review, vol. 11, no. 2, pp. 175–201, 2021.\\n\\n[55] N. Charness and W. R. Boot, “Aging and information technology use: Potential and barriers,” Current directions\\n\\nin psychological science, vol. 18, no. 5, pp. 253–258, 2009.\\n\\n[56] I. Iancu and B. Iancu, “Designing mobile technology for elderly. a theoretical overview,” Technological Forecast-\\n\\ning and Social Change, vol. 155, p. 119977, 2020.\\n\\n[57] N. Charness and W. R. Boot, “Aging and information technology use: Potential and barriers,” Current directions\\n\\nin psychological science, vol. 18, no. 5, pp. 253–258, 2009.\\n\\n[58] Y.-R. R. Chen and P. J. Schulz, “The effect of information communication technology interventions on reducing social isolation in the elderly: a systematic review,” Journal of medical Internet research, vol. 18, no. 1, p. e4596, 2016.\\n\\n[59] B. Barbosa Neves, R. Franz, R. Judges, C. Beermann, and R. Baecker, “Can digital technology enhance social connectedness among older adults? a feasibility study,” Journal of Applied Gerontology, vol. 38, no. 1, pp. 49–72, 2019.\\n\\n[60] E. Vaportzis, M. Giatsi Clausen, and A. J. Gow, “Older adults perceptions of technology and barriers to interacting\\n\\nwith tablet computers: a focus group study,” Frontiers in psychology, vol. 8, p. 1687, 2017.\\n\\n[61] H. Petrie, “Accessibility and usability requirements for icts for disabled and elderly people: a functional\\n\\nclassification approach,” 2001.\\n\\n[62] H. M. Mohadisdudis and N. M. Ali, “A study of smartphone usage and barriers among the elderly,” in 2014 3rd\\n\\ninternational conference on user science and engineering (i-USEr), pp. 109–114, IEEE, 2014.\\n\\n[63] N. T. Msweli and T. Mawela, “Enablers and barriers for mobile commerce and banking services among the elderly in developing countries: a systematic review,” in Responsible Design, Implementation and Use of Information and Communication Technology: 19th IFIP WG 6.11 Conference on e-Business, e-Services, and e-Society, I3E 2020, Skukuza, South Africa, April 6–8, 2020, Proceedings, Part II 19, pp. 319–330, Springer, 2020.\\n\\n[64] T. L. Mitzner, J. B. Boron, C. B. Fausset, A. E. Adams, N. Charness, S. J. Czaja, K. Dijkstra, A. D. Fisk, W. A. Rogers, and J. Sharit, “Older adults talk technology: Technology usage and attitudes,” Computers in human behavior, vol. 26, no. 6, pp. 1710–1721, 2010.\\n\\n[65] J. T. Cacioppo and S. Cacioppo, “Social relationships and health: The toxic effects of perceived social isolation,”\\n\\nSocial and personality psychology compass, vol. 8, no. 2, pp. 58–72, 2014.\\n\\n[66] T. Sims, A. E. Reed, and D. C. Carr, “Information and communication technology use is related to higher well-being among the oldest-old,” Journals of Gerontology Series B: Psychological Sciences and Social Sciences, vol. 72, no. 5, pp. 761–770, 2017.\\n\\n[67] J. Leibold, “Surveillance in china’s xinjiang region: Ethnic sorting, coercion, and inducement,” Journal of\\n\\ncontemporary China, vol. 29, no. 121, pp. 46–60, 2020.\\n\\n[68] C. Buckley and P. Mozur, “How china uses high-tech surveillance to subdue minorities,” New York Times, vol. 22,\\n\\n2019.\\n\\n[69] Z. Su, A. Cheshmehzangi, D. McDonnell, B. L. Bentley, C. P. Da Veiga, and Y.-T. Xiang, “Facial recognition\\n\\nlaw in china,” Journal of Medical Ethics, vol. 48, no. 12, pp. 1058–1059, 2022.\\n\\n[70] R. Creemers, “China’s social credit system: an evolving practice of control,” Available at SSRN 3175792, 2018. [71] S. Zuboff, “Big other: surveillance capitalism and the prospects of an information civilization,” Journal of\\n\\ninformation technology, vol. 30, no. 1, pp. 75–89, 2015.\\n\\n[72] C. Bartneck, C. Lütge, A. Wagner, and S. Welsh, An introduction to ethics in robotics and AI. Springer Nature,\\n\\n2021.\\n\\n[73] H. Field, “Zoom can now train its a.i. using some customer data, according to updated terms,” August 2023. [74] S. K. Jha, S. Jha, and M. K. Gupta, “Leveraging artificial intelligence for effective recruitment and selection processes,” in International Conference on Communication, Computing and Electronics Systems: Proceedings of ICCCES 2019, pp. 287–293, Springer, 2020.\\n\\n[75] J. Dastin, “Amazon scraps secret ai recruiting tool that showed bias against women,” in Ethics of data and\\n\\nanalytics, pp. 296–299, Auerbach Publications, 2022.\\n\\n25\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n[76] D. Harwell, “A face-scanning algorithm increasingly decides whether you deserve the job,” Ethics of Data and\\n\\nAnalytics, pp. 206–211, 2019.\\n\\n[77] Z. F. Khan and S. R. Alotaibi, “Applications of artificial intelligence and big data analytics in m-health: a\\n\\nhealthcare system perspective,” Journal of healthcare engineering, vol. 2020, pp. 1–15, 2020.\\n\\n[78] H. Malik, N. Fatema, and J. A. Alzubi, AI and machine learning paradigms for health monitoring system:\\n\\nintelligent data analytics, vol. 86. Springer Nature, 2021.\\n\\n[79] E. J. Topol, “High-performance medicine: the convergence of human and artificial intelligence,” Nature medicine,\\n\\nvol. 25, no. 1, pp. 44–56, 2019.\\n\\n[80] L. Coventry and D. Branley, “Cybersecurity in healthcare: A narrative review of trends, threats and ways forward,”\\n\\nMaturitas, vol. 113, pp. 48–52, 2018.\\n\\n[81] V. Liu, M. A. Musen, and T. Chou, “Data breaches of protected health information in the united states,” Jama,\\n\\nvol. 313, no. 14, pp. 1471–1473, 2015.\\n\\n[82] M. A. Rothstein, “Is deidentification sufficient to protect health privacy in research?,” The American Journal of\\n\\nBioethics, vol. 10, no. 9, pp. 3–11, 2010.\\n\\n[83] W. Raghupathi and V. Raghupathi, “Big data analytics in healthcare: promise and potential,” Health information\\n\\nscience and systems, vol. 2, pp. 1–10, 2014.\\n\\n[84] D. G. Muntinga, M. Moorman, and E. G. Smit, “Introducing cobras: Exploring motivations for brand-related\\n\\nsocial media use,” International Journal of advertising, vol. 30, no. 1, pp. 13–46, 2011.\\n\\n[85] A. J. Jeckmans, M. Beye, Z. Erkin, P. Hartel, R. L. Lagendijk, and Q. Tang, “Privacy in recommender systems,”\\n\\nSocial media retrieval, pp. 263–281, 2013.\\n\\n[86] P. Glynn, “Sony world photography award 2023: Winner refuses award after revealing ai creation,” BBC News,\\n\\n2023.\\n\\n[87] M. Westerlund, “The emergence of deepfake technology: A review,” Technology innovation management review,\\n\\nvol. 9, no. 11, 2019.\\n\\n[88] B. Chesney and D. Citron, “Deep fakes: A looming challenge for privacy, democracy, and national security,”\\n\\nCalif. L. Rev., vol. 107, p. 1753, 2019.\\n\\n[89] O. Osoba and W. Welser IV, “An intelligence in our image,” Santa Mônica: RAND corporation, 2017. [90] G. Spitale, N. Biller-Andorno, and F. Germani, “Ai model gpt-3 (dis) informs us better than humans,” arXiv\\n\\npreprint arXiv:2301.11924, 2023.\\n\\n[91] S. Vosoughi, D. Roy, and S. Aral, “The spread of true and false news online,” science, vol. 359, no. 6380,\\n\\npp. 1146–1151, 2018.\\n\\n[92] G. Pennycook, A. Bear, E. T. Collins, and D. G. Rand, “The implied truth effect: Attaching warnings to a subset of fake news headlines increases perceived accuracy of headlines without warnings,” Management science, vol. 66, no. 11, pp. 4944–4957, 2020.\\n\\n[93] A. Ali, K. F. K. Ghouri, H. Naseem, T. R. Soomro, W. Mansoor, and A. M. Momani, “Battle of deep fakes: Artificial intelligence set to become a major threat to the individual and national security,” in 2022 International Conference on Cyber Resilience (ICCR), pp. 1–5, IEEE, 2022.\\n\\n[94] J. Bateman, Deepfakes and synthetic media in the financial system: Assessing threat scenarios. Carnegie\\n\\nEndowment for International Peace., 2020.\\n\\n[95] C. Stupp, “Fraudsters used ai to mimic ceo’s voice in unusual cybercrime case,” The Wall Street Journal, vol. 30,\\n\\nno. 08, 2019.\\n\\n[96] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndi´c, P. Laskov, G. Giacinto, and F. Roli, “Evasion attacks against machine learning at test time,” in Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13, pp. 387–402, Springer, 2013.\\n\\n[97] T. C. King, N. Aggarwal, M. Taddeo, and L. Floridi, “Artificial intelligence crime: An interdisciplinary analysis\\n\\nof foreseeable threats and solutions,” Science and engineering ethics, vol. 26, pp. 89–120, 2020.\\n\\n[98] A. Azzutti, W.-G. Ringe, and H. S. Stiehl, “Machine learning, market manipulation, and collusion on capital\\n\\nmarkets: Why the\" black box\" matters,” U. Pa. J. Int’l L., vol. 43, p. 79, 2021.\\n\\n[99] Financial Industry Regulatory Authority (FINRA), “Artificial intelligence (ai) in the securities industry,” tech.\\n\\nrep., Financial Industry Regulatory Authority, 2020.\\n\\n26\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n[100] P. Coffee, “Deepfakes of celebrities have begun appearing in ads, with or without their permission,” The Wall\\n\\nStreet Journal, 2022. Retrieved March 27, 2022.\\n\\n[101] A. Satariano and P. Mozur, “The people onscreen are fake. the disinformation is real,” The New York Times, 2023.\\n\\nRetrieved March 27, 2023.\\n\\n[102] N. Bostrom, Superintelligence: Paths, Dangers, Strategies. 2014.\\n\\n[103] A. Griffin, “Facebook’s artificial intelligence robots shut down after they start talking to each other in their own\\n\\nlanguage,” Independent, 2017.\\n\\n[104] G. staff, “Us air force denies running simulation in which ai drone ‘killed’ operator,” The Guardian, June 2023.\\n\\n[105] R. Arkin, Governing Lethal Behavior in Autonomous Robots. CRC Press, 2009.\\n\\n[106] R. Daneshjou and et al., “Lack of transparency and potential bias in artificial intelligence data sets and algorithms:\\n\\nA scoping review,” JAMA Dermatology, vol. 157, no. 11, pp. 1362–1369, 2021.\\n\\n[107] L. Sweeney, “Discrimination in online ad delivery,” Communications of the ACM, vol. 56, no. 5, pp. 44–54, 2013.\\n\\n[108] J. Buolamwini and T. Gebru, “Gender shades: Intersectional accuracy disparities in commercial gender classifi-\\n\\ncation,” in Conference on Fairness, Accountability, and Transparency, PMLR, 2018.\\n\\n[109] J. Angwin and et al., “Machine bias,” 2016.\\n\\n[110] J. Burrell, “How the machine ‘thinks’: Understanding opacity in machine learning algorithms,” Big Data &\\n\\nSociety, vol. 3, no. 1, p. 2053951715622512, 2016.\\n\\n[111] C. L. Devane and et al., “Anxiety disorders in the 21st century: Status, challenges, opportunities, and comorbidity\\n\\nwith depression,” The American Journal of Managed Care, vol. 11, no. 12, pp. S344–53, 2005.\\n\\n[112] K. P. Conway and et al., “Lifetime comorbidity of dsm-iv mood and anxiety disorders and specific drug use disorders: Results from the national epidemiologic survey on alcohol and related conditions,” Journal of Clinical Psychiatry, vol. 67, no. 2, pp. 247–257, 2006.\\n\\n[113] J. T. Cacioppo and et al., “Loneliness as a specific risk factor for depressive symptoms: Cross-sectional and\\n\\nlongitudinal analyses,” Psychology and Aging, vol. 21, no. 1, p. 140, 2006.\\n\\n[114] M. Kalia, “Neurobiological basis of depression: An update,” Metabolism, vol. 54, no. 5, pp. 24–27, 2005.\\n\\n[115] T. Roth, “Insomnia: Definition, prevalence, etiology, and consequences,” Journal of Clinical Sleep Medicine,\\n\\nvol. 3, no. 5, pp. S7–S10, 2007.\\n\\n[116] M. P. Walker, “The role of sleep in cognition and emotion,” Annals of the New York Academy of Sciences,\\n\\nvol. 1156, no. 1, pp. 168–197, 2009.\\n\\n[117] Y. Liu, Z. Wang, and W. Lü, “Resilience and affect balance as mediators between trait emotional intelligence and\\n\\nlife satisfaction,” Personality and Individual Differences, vol. 54, no. 7, pp. 850–855, 2013.\\n\\n[118] R. Sinha, “Chronic stress, drug use, and vulnerability to addiction,” Annals of the New York Academy of Sciences,\\n\\nvol. 1141, no. 1, pp. 105–130, 2008.\\n\\n[119] T. C. Adam and E. S. Epel, “Stress, eating, and the reward system,” Physiology & Behavior, vol. 91, no. 4,\\n\\npp. 449–458, 2007.\\n\\n[120] M. Kivimäki and et al., “Job strain as a risk factor for coronary heart disease: A collaborative meta-analysis of\\n\\nindividual participant data,” The Lancet, vol. 380, no. 9852, pp. 1491–1497, 2012.\\n\\n[121] S. Cohen and et al., “Chronic stress, glucocorticoid receptor resistance, inflammation, and disease risk,” Proceed-\\n\\nings of the National Academy of Sciences, vol. 109, no. 16, pp. 5995–5999, 2012.\\n\\n[122] M. D. Marciniak and et al., “The cost of treating anxiety: The medical and demographic correlates that impact\\n\\ntotal medical costs,” Depression and Anxiety, vol. 21, no. 4, pp. 178–184, 2005.\\n\\n[123] A. M. Roest and et al., “Anxiety and risk of incident coronary heart disease: A meta-analysis,” Journal of the\\n\\nAmerican College of Cardiology, vol. 56, no. 1, pp. 38–46, 2010.\\n\\n[124] M. W. Eysenck and et al., “Anxiety and cognitive performance: Attentional control theory,” Emotion, vol. 7,\\n\\nno. 2, p. 336, 2007.\\n\\n[125] S. De Zordo, D. Marre, and M. Smietana, “Demographic anxieties in the age of ‘fertility decline’,” Medical\\n\\nAnthropology, vol. 41, no. 6-7, pp. 591–599, 2022.\\n\\n[126] H. Fallahzadeh and et al., “The comparison of depression and anxiety between fertile and infertile couples: A meta-analysis study,” International Journal of Reproductive Biomedicine, vol. 17, no. 3, p. 153, 2019.\\n\\n27\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n[127] G. M. B. Louis and et al., “Stress reduces conception probabilities across the fertile window: Evidence in support\\n\\nof relaxation,” Fertility and Sterility, vol. 95, no. 7, pp. 2184–2189, 2011.\\n\\n[128] C. D. Lynch and et al., “Preconception stress increases the risk of infertility: Results from a couple-based\\n\\nprospective cohort study—the life study,” Human Reproduction, vol. 29, no. 5, pp. 1067–1075, 2014.\\n\\n[129] S. Whirledge and J. A. Cidlowski, “Glucocorticoids, stress, and fertility,” Minerva Endocrinologica, vol. 35,\\n\\nno. 2, p. 109, 2010.\\n\\n[130] V. H. Nargund, “Effects of psychological stress on male fertility,” Nature Reviews Urology, vol. 12, no. 7,\\n\\npp. 373–382, 2015.\\n\\n[131] J. C. Wingfield and R. M. Sapolsky, “Reproduction and resistance to stress: When and how,” Journal of\\n\\nNeuroendocrinology, vol. 15, no. 8, pp. 711–724, 2003.\\n\\n[132] H. Colleran and et al., “Fertility decline and the changing dynamics of wealth, status, and inequality,” Proceedings\\n\\nof the Royal Society B: Biological Sciences, vol. 282, no. 1806, p. 20150287, 2015.\\n\\n[133] J. Torous and L. W. Roberts, “Needed innovation in digital health and smartphone applications for mental health:\\n\\nTransparency and trust,” JAMA Psychiatry, vol. 74, no. 5, pp. 437–438, 2017.\\n\\n[134] D. D. Luxton, “Artificial intelligence in psychological practice: Current and future applications and implications,”\\n\\nProfessional Psychology: Research and Practice, vol. 45, no. 5, p. 332, 2014.\\n\\n[135] A. T. Beck, “Cognitive therapy: A 30-year retrospective,” American Psychologist, vol. 46, no. 4, p. 368, 1991.\\n\\n[136] T. Donker and et al., “Psychoeducation for depression, anxiety, and psychological distress: A meta-analysis,”\\n\\nBMC Medicine, vol. 7, no. 1, pp. 1–9, 2009.\\n\\n[137] K. M. Connor and J. R. Davidson, “Development of a new resilience scale: The connor-davidson resilience scale\\n\\n(cd-risc),” Depression and Anxiety, vol. 18, no. 2, pp. 76–82, 2003.\\n\\n[138] D. Freeman and et al., “Virtual reality in the assessment, understanding, and treatment of mental health disorders,”\\n\\nPsychological Medicine, vol. 47, no. 14, pp. 2393–2400, 2017.\\n\\n[139] J. N. Butcher and K. Hostetler, “Abbreviating mmpi item administration: What can be learned from the mmpi for the mmpi—2?,” Psychological Assessment: A Journal of Consulting and Clinical Psychology, vol. 2, no. 1, p. 12, 1990.\\n\\n[140] M. Weller, “Twenty years of edtech,” Educause Review Online, vol. 53, no. 4, pp. 34–48, 2018.\\n\\n[141] K. Roose, “Don’t ban chatgpt in schools. teach with it.,” The New York Times, Jan 2023.\\n\\n[142] H. Partovi, “Schools and learning in an ai-enabled world,” Wall Street Journal, May 2023.\\n\\n[143] S. Khan, “How ai could save (not destroy) education,” Apr 2023.\\n\\n[144] L. Floridi and J. Cowls, “A unified framework of five principles for ai in society,” Machine Learning and the\\n\\nCity: Applications in Architecture and Urban Design, pp. 535–545, 2022.\\n\\n[145] N. A. Smuha, “The eu approach to ethics guidelines for trustworthy artificial intelligence,” Computer Law Review\\n\\nInternational, vol. 20, no. 4, pp. 97–106, 2019.\\n\\n[146] K. Yeung, A. Howes, and G. Pogrebna, “Ai governance by human rights–centered design, deliberation, and\\n\\noversight,” in The Oxford Handbook of Ethics of AI, pp. 77–106, 2020.\\n\\n[147] F. M. Zanzotto, “Human-in-the-loop artificial intelligence,” Journal of Artificial Intelligence Research, vol. 64,\\n\\npp. 243–252, 2019.\\n\\n[148] S. Russell, Human Compatible: Artificial Intelligence and the Problem of Control. Penguin, 2019.\\n\\n[149] G. Pullin and A. Newell, “Focusing on extraordinary users,” in Universal Access in Human Computer Interaction. Coping with Diversity: 4th International Conference on Universal Access in Human-Computer Interaction, UAHCI 2007, Held as Part of HCI International 2007, Beijing, China, July 22-27, 2007, Proceedings, Part I, 2007.\\n\\n[150] D. Pal and et al., “User experience with smart voice assistants: The accent perspective,” in 2019 10th International\\n\\nConference on Computing, Communication and Networking Technologies (ICCCNT), IEEE, 2019.\\n\\n[151] K. Roose, “Ai poses ‘risk of extinction,’ industry leaders warn,” The New York Times, 2023.\\n\\n[152] E. Johnson, “Hr 6216 - national artificial intelligence initiative act of 2020,” 2022.\\n\\n[153] J. Mökander et al., “The us algorithmic accountability act of 2022 vs. the eu artificial intelligence act: what can\\n\\nthey learn from each other?,” Minds and Machines, vol. 32, no. 4, pp. 751–758, 2022.\\n\\n28\\n\\nAI Anxiety: A Comprehensive Analysis of Psychological Factors and Interventions\\n\\nA PREPRINT\\n\\n[154] T. Kannampallil et al., “Effects of a virtual voice-based coach delivering problem-solving treatment on emotional distress and brain function: a pilot rct in depression and anxiety,” Translational Psychiatry, vol. 13, no. 1, p. 166, 2023.\\n\\n[155] T. H. Davenport and R. Ronanki, “Artificial intelligence for the real world,” Harvard Business Review, vol. 96,\\n\\nno. 1, pp. 108–116, 2018.\\n\\n[156] J. W. Gichoya et al., “Ai recognition of patient race in medical imaging: a modeling study,” The Lancet Digital\\n\\nHealth, vol. 4, no. 6, pp. e406–e414, 2022.\\n\\n29\\n\\nView publication stats'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview first page\n",
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faacc1-be29-4d52-a46e-94f5b5b8e728",
   "metadata": {},
   "source": [
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dcf2cfe-a7aa-4ecf-85e3-f77b9e850514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39aebbf8-92bf-42e5-951e-40bb458852d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   \tID          \tSIZE  \tMODIFIED               \n",
      "mistral:latest         \t61e88e884507\t4.1 GB\t21 minutes ago        \t\n",
      "nomic-embed-text:latest\t0a109f422b47\t274 MB\tLess than a second ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5394d61f-906b-4776-b8b5-9f0045c76193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q chromadb\n",
    "%pip install --q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a39856-0cc0-4ebe-8024-9db32455a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad040e2-3abe-4e23-abb9-951b223b9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efb11c92-e732-4a88-8f57-57a19b38e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 14/14 [00:40<00:00,  2.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# Add to vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True),\n",
    "    collection_name=\"local-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eadf50-2f3d-4420-8858-94e9c1682ffa",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ec338c4-f282-462f-b0a0-c1899538eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1d6ceeb-6883-4688-b923-e771c2b2cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"mistral\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c436d5cd-5dd0-448c-b5c0-6eddab879c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71e423dc-f632-46f8-9bec-d74cb268ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(),\n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb1f308f-8472-4506-9517-d79b61d408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c25c1d-d205-409e-90a2-179d0bd7c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.36it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The article proposes several solutions and mitigation measures to address AI anxiety:\\n\\n1. AI-adapted clinical guidelines: Developing tailored diagnostic and therapeutic measures aimed at mitigating AI anxiety for different age groups is essential. These guidelines should be updated regularly to account for the evolving nature of AI technology.\\n2. Continuous education: Updating school curricula to incorporate AI technology and its implications is crucial to help individuals adapt to the changing technological landscape. This includes demystifying AI and promoting responsible usage.\\n3. Regulatory measures: Balancing AI innovation with societal adaptability requires regulatory measures. Engineering constraints and ethical design are essential components of responsible AI development.\\n4. Mental health interventions: Leveraging AI in mental health interventions can help mitigate AI anxiety by providing accessible, personalized, and effective treatments for individuals experiencing anxiety related to AI.\\n\\nThe article emphasizes the need for intensified focus and strategic resource allocation to effectively address the escalating mental health challenges as society navigates the era of pervasive AI.\\n\\nReferences: [1] Mccarthy, J., Lispitz, M. A., & Michalski, Z. (1956). Programs with common sense. Proceedings of the 1st Berkeley Symposium on Mathematical Statistics and Probability, 2, 548-573.\\n[2] Graves, A., Jaitly, N., & Kingsbury, B. (2013). Speech recognition with deep recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 3781-3788).\\n[3] Vaswani, A., Shlehaut, M., Parmar, N., Uszkoreit, J., Jones, L., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30, 6048-6058.\\n[4] Brown, A., Schoenebeck, J., & Zou, J. Y. (2020). Language models are unintended autoregressive text generators. arXiv preprint arXiv:2007.11576.\\n[5] Russell, S. J., & Norvig, P. (2009). Artificial intelligence: A modern approach (3rd ed.). Pearson Education India.\\n[6] Szeliski, R., Pietro, M., & Leibe, B. (2010). A survey on object recognition. IEEE Transactions on pattern analysis and machine intelligence, 32(8), 1597-1622.\\n[7] Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A. R., Jaitly, N., ... & Kingsbury, B. (2012). Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal processing magazine, 29(6), 82-97.\\n[8] LeCun, Y., Bengio, S., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.\\n[9] LeCun, Y., Boser, B. E., Denil, P., Deng, L., Dahl, G. E., Mohamed, A. R., ... & Kingsbury, B. (2015). Deep residual learning for image recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1781-1793.\\n[10] Haykin, S. (1998). Neural networks: Theory and applications. John Wiley & Sons.\\n[11] Szmalowski, K., & Grzesik, M. (2014). Handcrafted feature extraction methods in object recognition: A survey. In Proceedings of the 23rd international conference on image processing (ICIP) (pp. 315-324).\\n[12] LeCun, Y., Bengio, S., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.\\n[13] Goodfellow, I., Bengio, S., Courville, A., & Vincent, Y. (2016). Deep learning. MIT press.\\n[14] Montavon, R. M. (2018). Neural networks for speech recognition: From traditional to deep learning. In Proceedings of the IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 9377-9381).'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are potential solutions and mitigation measures to AI anxiety?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfe79f21-48aa-4820-aa9f-79f3d1a0a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all collections in the db\n",
    "vector_db.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
